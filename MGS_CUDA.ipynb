{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MGS_CUDA.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"WV6t3ajrmChB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621187834425,"user_tz":-330,"elapsed":8379,"user":{"displayName":"Abhijeet Bodas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMC9fqDaOIIZLhKu3x-y8xz6mvH65iHBkaV8LVs9M=s64","userId":"10865055610535196293"}},"outputId":"96c69be0-2875-4070-ac68-61af869f7635"},"source":["# Skip the below code block if running on a local CUDA installation.\n","!nvcc --version\n","!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n","%load_ext nvcc_plugin"],"execution_count":1,"outputs":[{"output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2020 NVIDIA Corporation\n","Built on Wed_Jul_22_19:09:09_PDT_2020\n","Cuda compilation tools, release 11.0, V11.0.221\n","Build cuda_11.0_bu.TC445_37.28845127_0\n","Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n","  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-uzn25plf\n","  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-uzn25plf\n","Building wheels for collected packages: NVCCPlugin\n","  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp37-none-any.whl size=4307 sha256=48c2b23a0bd0aba70036f54e7491e1ab3d1508f1cf768a64d08666d62f298e33\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-zltin_jx/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n","Successfully built NVCCPlugin\n","Installing collected packages: NVCCPlugin\n","Successfully installed NVCCPlugin-0.0.2\n","created output directory at /content/src\n","Out bin /content/result.out\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Up1xzXn2unp","executionInfo":{"status":"ok","timestamp":1621187847012,"user_tz":-330,"elapsed":8169,"user":{"displayName":"Abhijeet Bodas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMC9fqDaOIIZLhKu3x-y8xz6mvH65iHBkaV8LVs9M=s64","userId":"10865055610535196293"}},"outputId":"2f6665dd-9866-4bef-e998-1a7c4fe91e2c"},"source":["%%cu\n","// Remove the above \"%%cu\" if running on a local CUDA installation, instead of a Google colab notebook.\n","\n","/*\n","    Parallelized Modified Gram Schmidt algorithm for QR decomposition of matrix.\n","    Authored-by: Abhijeet Prasad Bodas, Indian Institute of Technology Bombay\n","    Environment: Google Colab notebook\n","    CUDA version: v11.0.221\n","    GPU: Tesla T4\n","*/\n","\n","\n","#include <bits/stdc++.h>\n","using namespace std;\n","\n","// Dimension of the matrix.\n","extern const int N = 3;\n","\n","void print_matrix(double *m)\n","{\n","    // Prints a given N*N matrix.\n","    cout << endl;\n","    for (int i = 0; i < N; i++)\n","    {\n","        for (int j = 0; j < N; j++)\n","        {\n","            printf(\"%5.4f \", m[i * N + j]);\n","        }\n","        cout << endl;\n","    }\n","}\n","\n","void initialize_matrix(double *m)\n","{\n","    // Initialize a N*N matrix with random values.\n","\n","    // **Note**: The QR method requires the matrix (A) to have linearly independent\n","    // columns. With a random initialization, there is a (pretty rare) chance that\n","    // this may not be satisfied.\n","    for (int q = 0; q < N * N; q++)\n","    {\n","        // m[q] = (double) rand() / RAND_MAX * 100;\n","        m[q] = (double) 10 / sqrt(q + 1);\n","    }\n","}\n","\n","/*\n","    Common variable naming scheme used in all kernels:\n","\n","    `column_index`:\n","    The index of the column in the array. The first column will have\n","    `column_index` = 0, and the last one will have `column_index` = N - 1.\n","    This is used when only a single column is used in the kernel.\n","\n","    `previous_column_index` and `current_column_index`: \"Previous\" denotes the column vector\n","    **on** which the projections are being made, and \"current\" denotes the column\n","    vector **of** which the projections are being taken.\n","\n","    `element_in_current_column_index`:\n","    The index of the element in the current column. This is also the index in the previous\n","    column, because all calculations are between corresponding elements!\n","    This will usually be the same as the thread ID.\n","*/\n","\n","__global__ void innerprod_self(double *m, double *result, int column_index)\n","{\n","    // Calculate the 2-norm of column vector of a matrix (m), and store it in result.\n","\n","    __shared__ double single_squares[N];\n","    int element_in_column_index = threadIdx.x;\n","    single_squares[element_in_column_index] = pow(m[column_index + element_in_column_index * N], 2);\n","\n","    // Proceed only after all threads have completed above calculations.\n","    __syncthreads();\n","\n","    if (element_in_column_index == 0)\n","    {\n","        // There is nothing special about `element_in_column_index` = 0. We only want to make\n","        // sure that we do the following calculation only once.\n","\n","        double temp = 0;\n","        for (int e = 0; e < N; e++)\n","        {\n","            temp = temp + single_squares[e];\n","        }\n","\n","        // Store the result.\n","        result[0] = sqrt(temp);\n","    }\n","}\n","\n","__global__ void scale(double *m, double *val, int column_index)\n","{\n","    // Used to normalize a column vector, by passing the norm of it in `val`.\n","    // Will divide all elements of the column vector with `val`.\n","\n","    int element_in_column_index = threadIdx.x;\n","    m[column_index + element_in_column_index * N] = m[column_index + element_in_column_index * N] / val[0];\n","}\n","\n","__global__ void  calculate_coefficients(double *m, double *coefficients, int previous_column_index)\n","{\n","    /*\n","        Calculates the dot products of previous_column with each of the columns of the\n","        matrix after the current column, and stores them in the the `coefficients` array.\n","        Naturally, the `coefficients` array is of length (N - previous_column_index).\n","\n","        Note that, because the `previous_column` has already been normalized, these\n","        dot products are also the lengths of the projections of the column vectors after\n","        the `previous_column` on the `previous_columns`. Hence, they will be used as coefficients\n","        to multiply the `previous_column` with, to get the projection vectors, which we will be\n","        later subtracting from the column vectors after the `previous_column`.\n","    */\n","    __shared__ double prod[N];\n","\n","\n","    // When the previous column index is `w`, we will need to calculate (N-w coefficients), and will have\n","    // assigned one block to calculation of each coefficient. The blockID is the index of the current column\n","    // vector **relative to the previous column vector**. So, the column just next to the `previous_column`\n","    // will have blockIdx = 0, and so on. So, we need to calculate the absolute index of the current column\n","    // separately.\n","    int current_column_relative_index = blockIdx.x;\n","    int current_column_absolute_index = current_column_relative_index + previous_column_index + 1;\n","\n","    int element_in_current_column_index = threadIdx.x;\n","    prod[element_in_current_column_index] = m[previous_column_index + element_in_current_column_index * N] * m[current_column_absolute_index + element_in_current_column_index * N]; // (element of previous col vector) * (element of current column vector)\n","    __syncthreads();\n","\n","    if (element_in_current_column_index == 0)\n","    {\n","        double temp;\n","        temp = 0;\n","        for (int e = 0; e < N; e++)\n","        {\n","            temp = temp + prod[e];\n","        }\n","        coefficients[current_column_relative_index] = temp;\n","    }\n","}\n","\n","__global__ void subtract_projections(double *m, double *coefficients, int previous_column_index)\n","{\n","    // Given the projection lengths (coefficients) and the (normalized) previous column,\n","    // subtract the projections.\n","    int element_in_current_column_index = threadIdx.x;\n","    int current_column_relative_index = blockIdx.x;\n","    int current_column_absolute_index = current_column_relative_index + previous_column_index + 1;\n","\n","    m[current_column_absolute_index + element_in_current_column_index * N] = m[current_column_absolute_index + element_in_current_column_index * N] - coefficients[current_column_relative_index] * m[previous_column_index + element_in_current_column_index * N];\n","}\n","\n","__global__ void multiply_transpose(double *q, double *a, double *r)\n","{\n","    // Stores Q_transpose * A in R\n","\n","    int block = blockIdx.x;\n","    int thread = threadIdx.x;\n","\n","    r[block * N + thread] = 0;\n","    for (int i = 0; i < N; i++)\n","    {\n","        // Multiply column of Q with another column of A, which has the same effect\n","        // as multiplying Q_transpose and A.\n","        r[block * N + thread] = r[block * N + thread] + q[i * N + block] * a[i * N + thread];\n","    }\n","}\n","\n","int main(void)\n","{\n","    // Allocate memory and initialize the matrix A on the host (CPU).\n","    double *A_host = (double *)malloc(N * N * sizeof(double));\n","    double *Q_host = (double *)malloc(N * N * sizeof(double));\n","    double *R_host = (double *)malloc(N * N * sizeof(double));\n","    initialize_matrix(A_host);\n","\n","    // Store A on the device.\n","    double *A_device;\n","    cudaMalloc((void **)&A_device, (N * N) * sizeof(double));\n","    cudaMemcpy(A_device, A_host, (N * N) * sizeof(double), cudaMemcpyHostToDevice);\n","\n","\n","    // Allocate memory for Q in device (GPU).\n","    double *Q_device;\n","    cudaMalloc((void **)&Q_device, (N * N) * sizeof(double));\n","    // Initialize Q on the device to A. After all operations, it will be transformed\n","    // into a orthonormal matrix.\n","    cudaMemcpy(Q_device, A_host, (N * N) * sizeof(double), cudaMemcpyHostToDevice);\n","\n","    // Allocate memory for R on device.\n","    double *R_device;\n","    cudaMalloc((void **)&R_device, (N * N) * sizeof(double));\n","\n","    // This is a temporary variable which will be used to store the norms of previous vectors.\n","    double *norm_device;\n","    cudaMalloc((void **)&norm_device, sizeof(double));\n","\n","    // 3.. 2.. 1.. GO!\n","    auto start_time = std::chrono::high_resolution_clock::now();\n","\n","    for (int w = 1; w < N; w++)\n","    {\n","        int column_index = w - 1;\n","\n","        // Calculate norm of previous column vector.\n","        innerprod_self<<<1, N>>>(Q_device, norm_device, column_index);\n","\n","        // Normalize the previous column vector.\n","        scale<<<1, N>>>(Q_device, norm_device, column_index);\n","\n","        double *coefficients_device;\n","        int number_of_coefficients = N - w;\n","        cudaMalloc((void **)&coefficients_device, (N - w) * sizeof(double));\n","\n","        // Calculate coefficients (projection lengths) for columns w to N-1.\n","        calculate_coefficients<<<number_of_coefficients, N>>>(Q_device, coefficients_device, column_index);\n","\n","        // Subtract projections of the previous column vectors from columns w to N-1.\n","        subtract_projections<<<number_of_coefficients, N>>>(Q_device, coefficients_device, column_index);\n","\n","        // Free memory.\n","        cudaFree(coefficients_device);\n","    }\n","\n","    // Normalize the last column, because the loop didn't do that.\n","    innerprod_self<<<1, N>>>(Q_device, norm_device, (N - 1));\n","    scale<<<1, N>>>(Q_device, norm_device, (N - 1));\n","\n","\n","    // Copy Q from device to host.\n","    cudaMemcpy(Q_host, Q_device, N * N * sizeof(double), cudaMemcpyDeviceToHost);\n","\n","    // Calculate R\n","    multiply_transpose<<<N, N>>>(Q_device, A_device, R_device);\n","    // Copy R from device to host.\n","    cudaMemcpy(R_host, R_device, N * N * sizeof(double), cudaMemcpyDeviceToHost);\n","\n","\n","    // Time up!\n","    auto end_time = std::chrono::high_resolution_clock::now();\n","    auto time_delta = std::chrono::duration_cast<std::chrono::nanoseconds>(end_time - start_time);\n","\n","    // Comment this out for large N's.\n","    cout<<endl<<\"Input matrix (A)\"<<endl;\n","    print_matrix(A_host);\n","    cout<<endl<<\"Output matrix (Q)\"<<endl;\n","    print_matrix(Q_host);\n","    cout<<endl<<\"Output matrix (R)\"<<endl;\n","    print_matrix(R_host);\n","\n","    // Print execution time.\n","    cout<<endl<<endl<<\"Time taken for decomposition (nanoseconds): \"<<time_delta.count()<<\"\\n\";\n","\n","    // Free memory.\n","    cudaFree(A_device);\n","    cudaFree(Q_device);\n","    cudaFree(R_device);\n","    cudaFree(norm_device);\n","    free(A_host);\n","    free(Q_host);\n","    free(R_host);\n","\n","    return 0;\n","}\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\n","Input matrix (A)\n","\n","10.0000 7.0711 5.7735 \n","5.0000 4.4721 4.0825 \n","3.7796 3.5355 3.3333 \n","\n","Output matrix (Q)\n","\n","0.8473 -0.5276 0.0607 \n","0.4237 0.6026 -0.6763 \n","0.3203 0.5988 0.7341 \n","\n","Output matrix (R)\n","\n","11.8019 9.0184 7.6891 \n","0.0000 1.0811 1.4098 \n","-0.0000 -0.0000 0.0363 \n","\n","\n","Time taken for decomposition (nanoseconds): 151322\n","\n"],"name":"stdout"}]}]}